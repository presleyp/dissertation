<<set-parent-cumulativity, echo=FALSE, cache=FALSE>>=
set_parent('diss.Rnw')
@

\section{Overview}
% One of the goals of phonology is to develop predictive models of phonotactics.
% Such models predict phonotactic judgments of words based on various properties
% of those words, UG, and the language's lexicon. In order to enable us to choose
% more effective models, I propose to investigate the correlation between the
% number of phonotactic violations in a word and the phonotactic judgment it
% elicits.

In modeling constraint-based phonotactics, there are three broad kinds of decisions to be made: which framework
to use, which parameters (constraints) to use, and how to tune (rank or weight) the parameters. This experiment
will weigh in on the question of framework choice, using the function they use to combine the effects of violations to distinguish among them.

Popular constraint-based frameworks include Optimality Theory \citep{Prince1993/2004}, Harmonic Grammar (taken here to mean linear Harmonic Grammar, in which harmony scores are not subject to exponentiation) \citep{Legendre1990c,Smolensky2006b,Pater2009,Potts2010}, and Maximum Entropy \citep{Goldwater2003}.

One feature of constraint-based frameworks is that they recognize units of violation that are independent of the words they appear in. In other words, two different words can be said to contain the same violation, and one word can be said to contain multiple different violations. Thus, these frameworks all have some method of combining units of violation into a grammaticality score for the entire word. However, they differ in how they define the combination operation. As a result, they make different predictions about the relative ungrammaticalities of words whose violations are in a subset-superset relationship. This experiment will seek to distinguish between frameworks on the basis of such groups of words.
%Ohala and Ohala 1986

% It may be linear, so that each additional violation of equal
% magnitude decreases judgments equally, or it may be nonlinear, so that
% additional violations have increasing or decreasing contributions to judgments.
\section{Optimality Theory}
Optimality Theory (OT) predicts that adding mild
violations to a word with a severe violation has no effect, so that the
function from number of violations to grammaticality is flat for any given
first violation as long as it remains one of the worst violations in the word. 
In other words, OT's function for combining the penalties of multiple violations 
is to return the maximum of those penalties as the penalty for the whole word 
(multiplied by the number of times that penalty is incurred).

Figure \ref{fig:OT-combiner} illustrates by crossing a strong violation, [mr] in the onset, with a weak violation,
[o:sp] in the rhyme. Each occurrence of the strong violation counts as a penalty of 2, while each occurrence of the weak
violation counts as a penalty of 1. The penalty of each word is simply the maximum of the penalties of the violations in the word, so that \textit{mroasp} is not any worse than \textit{mrote}. 

\begin{figure}
\begin{center}
\label{fig:OT-combiner}
<<OT-combiner, echo=FALSE>>=
par(mar = c(5.1, 4.1, 4.1, 4.1));
barplot(c(0, -1, -2, -2), main='Optimality Theory Violation Cumulativity', xlab='Penalty Vectors', ylab='Cumulative Penalty', names.arg=c("rote: [0, 0]", "roasp: [0, -1]", "mrote: [-2, 0]", "mroasp: [-2, -1]"), ylim=c(-3, 0));
@
\caption{Cumulativity in Optimality Theory.}
\end{center}
\end{figure}

However, predating OT, \citet{Ohala1986} found that speakers have an above
chance probability of preferring a word with one violation to a word with that
same violation and a less severe one, suggesting that even the milder violations
affect the grammaticality of a word. This contradicts OT's prediction that an additional
violation that is lesser than the first violation will not affect the ungrammaticality of the word.
Additionally, \citet{Coleman1997} found
that a word like \textit{mrupation}, with one severe violation followed by a
common English sequence, was preferred to a word like \textit{spleitisak}, with
several minor violations. This is in contrast with OT's prediction that 
the strong violation \textit{mr} matters
more than any number of lesser violations. 

% Albright:
% onsetviolation vs onsetandcodaviolation
% ratings
% model comparison
\citet{Albright_clusters_2008} designed experiments
to directly test the question of cumulativity of violations, addressing
potential alternative explanations for these two results, and found that models
that take into account all violations of a word, not just its worst violation,
fit the data significantly better. Albright used two types of words, those with
phonotactic violations in the onset and those with phonotactic violations in
the onset as well as milder violations in the rime. In a variety of analyses,
he fitted models that rate words by their worst violation only, and ones that
rate words by the sum of all their violations. The models that take into
account all violations in the word were more strongly correlated with experimental
findings. This study showed that cumulative models reflect speaker judgments better
than noncumulative models, but did not distinguish among various cumulative models.

I conclude that OT's strategy of combining violations by finding their maximum is
not empirically supported in the domain of phonotactics, and I turn to Harmonic Grammar and Maximum Entropy.

%Sorace and Keller 2004 found cumulativity of violations for syntax
%TODO discuss grammaticality vs acceptability vs ratings
\section{Linear vs. Exponential Combination}
\citet{Albright_clusters_2008} found evidence that all violations in a word contribute to the word's
ungrammaticality, but it 
did not compare various models that work this way against each other.
The shape of the curve relating number of violations to phonotactic judgments
bears on the question of which framework we should use to model phonotactic
well-formedness. As \citet{Pater_cumulative_2008} points out, Harmonic Grammar
predicts a well-restricted set of cumulativity effects, unlike Optimality
Theory with Local Constraint Conjunction \citep{Smolensky2006d}.
But the weighted constraints of Harmonic Grammar can be combined in a linear fashion,
producing the framework commonly associated with the name, or exponentiated and normalized,
as in Maximum Entropy. These approaches predict differently shaped curves.

\ex. Harmonic Grammar: The harmony $\mathcal{H}$ of a word $x$ is the dot product of the violation vector $v$,
representing violations of $x$ on each constraint in the constraint set $C$, with the constraint
weight vector $w$.\\
\[\mathcal{H}(x) = \sum_{i \in C}{v_iw_i}\]

\ex. Maximum Entropy: The probability $p$ of a word $x$ is the exponentiated negative harmony
of the word, normalized relative to the candidate set $X$.\\
\[p(x_i) = \frac{\exp(-\mathcal{H}(x_i))}{\sum_{j \in X}{\exp(-\mathcal{H}(x_j))}}\]

Consider a constraint with a weight
of two and candidates A, B, C, and D that violate it zero, one, two, and three
times respectively. In linear Harmonic Grammar, the candidates have the harmony scores
0, -2, -4, and -6; they decrease by two each time, in a linear pattern. In
Maximum Entropy, if we assume these candidates exhaust the possibilities, they
have the probabilities 0.865, 0.117, 0.015, and 0.002; candidate A has the
majority of the probability because it is the best choice available, and each
additional violation decreases the probability by a smaller amount than the
last.


\begin{figure}
\begin{center}
<<HG-combiner, echo=FALSE>>=
barplot(c(0, -1, -2, -3), main='Harmonic Grammar Violation Cumulativity', xlab='Penalty Vectors', ylab='Cumulative Penalty', names.arg=c("rote: [0, 0]", "roasp: [0, -1]", "mrote: [-2, 0]", "mroasp: [-2, -1]"), ylim=c(-3.5,0));
@
\caption{Cumulativity in Harmonic Grammar.}
\label{fig:HG-combiner}
\end{center}
\end{figure}
%\ex. \includegraphics[scale=.5]{hg_cumulativity.png}

\begin{figure}
\begin{center}
<<ME-combiner, echo=FALSE>>=
one = exp(0)
two = exp(-1)
three = exp(-2)
four = exp(-3)
barplot(c(one, two, three, four), main='Maximum Entropy Violation Cumulativity', xlab='Penalty Vectors', ylab='Cumulative Penalty', names.arg=c("rote: [0, 0]", "roasp: [0, 1]", "mrote: [2, 0]", "mroasp: [2, 1]"), ylim=c(0, 1.5));
@
\caption{Cumulativity in Maximum Entropy grammar.}
\label{fig:ME-combiner}
\end{center}
\end{figure}
%\ex. \includegraphics[scale=.5]{me_cumulativity.png}

<<HG-prob, echo=FALSE, eval=FALSE>>=
HG_prob = function(a, b){
  f <- function(x) dnorm(x, m=a, sd=1) - dnorm(x, m=b, sd=1)
  lower = min(a, b)
  higher = max(a, b)
  intersection = uniroot(f, interval=c(lower, higher))$root
  return(pnorm(intersection, mean=a, sd=1, lower.tail = FALSE))
}
HG_gg = HG_prob(0, -1)
HG_gb = HG_prob(-3, -1)
HG_bg = HG_prob(-2, -1)
HG_bb = HG_prob(-5, -1)
barplot(c(HG_gg, HG_bg, HG_gb, HG_bb))
@

<<HG-prob2, echo=FALSE, eval=FALSE>>=
intersection = function(g, lower, higher){
  return(uniroot(g, interval=c(lower, higher))$root)
}
prob_over = function(inter, mn, below){
  return(pnorm(inter, mean=mn, sd=0.5, lower.tail=below))
}
# filler: [0, 0, 1]
# gg: [0, 0, 0]
# gb: [0, 1, 0]
# bg: [1, 0, 0]
# bb: [1, 1, 0]
# weights: [-2, -3, -1] sd 0.5
# filler: [0, 0, (-1)] dnorm(x, mean=-1, sd=0.5)
# gg: [0, 0, 0] dnorm(x, mean=0, sd=0.5)
# gb: [0, -3, 0] dnorm(x, mean=-3, sd=0.5)
# bg: [-2, 0, 0] dnorm(x, mean=-2, sd=0.5)
# bb: [-2, -3, 0] (dnorm(x, mean=-2, sd=0.5) + dnorm(x, mean=-3, sd=0.5))
# gg over filler: dnorm(x, mean=0, sd=0.5) - dnorm(x, mean=-1, sd=0.5)
# gb over filler: dnorm(x, mean=-3, sd=0.5) - dnorm(x, mean=-1, sd=0.5)
# bg over filler: dnorm(x, mean=-2, sd=0.5) - dnorm(x, mean=-1, sd=0.5)
# bb over filler: (dnorm(x, mean=-2, sd=0.5) + dnorm(x, mean=-3, sd=0.5)) - dnorm(x, mean=-1, sd=0.5)

prob_filler = function(mn){
  inter = intersection(function(x){dnorm(x, mean = mn, sd = 0.5) - dnorm(x, mean = -1, sd = 0.5)}, mn, 0)
  prob_over_inter = prob_over(inter, -1, FALSE)
  return(prob_over_inter)
}

HG_prob_gg = prob_over(intersection(function(x){dnorm(x, mean=0, sd=0.5) - dnorm(x, mean=-1, sd=0.5)}, -1, 0), 0, FALSE)
HG_prob_gb = 1 - prob_filler(-3)
HG_prob_bg = 1 - prob_filler(-2)
HG_prob_bb = 1 - prob_over(intersection(function(x){(dnorm(x, mean=-2, sd=0.5) + dnorm(x, mean=-3, sd=0.5)) - dnorm(x, mean=-1, sd=0.5)}, -2.5, 0), -1, FALSE)
barplot(c(HG_prob_gg, HG_prob_bg, HG_prob_gb, HG_prob_bb))
@

<<HG-prob3, echo=FALSE, eval=FALSE>>=
weights = c(1, 2, 3)
sd = 1
hg_mean = function(violations){
  return(weights %*% violations)  
}

hg_variance = function(violations){
  return(rep(sd**2, length(violations)) %*% violations**2)  
}

# probability of choosing from the first dist
compare = function(violations1, violations2){
  mean1 = hg_mean(violations1)  
  mean2 = hg_mean(violations2)
  var1 = hg_variance(violations1)
  var2 = hg_variance(violations2)
  diff_mean = mean1 - mean2
  diff_var = var1 + var2
  return(pnorm(0, mean = diff_mean, sd = sqrt(diff_var), lower.tail=FALSE))
}

gg_violations = c(0, 0, 0)
bg_violations = c(0, -1, 0)
gb_violations = c(0, 0, -1)
bb_violations = c(0, -1, -1)
filler_violations = c(-1, 0, 0)
gg_filler = compare(gg_violations, filler_violations)
bg_filler = compare(bg_violations, filler_violations)
gb_filler = compare(gb_violations, filler_violations)
bb_filler = compare(bb_violations, filler_violations)
#same results as with robert's math
barplot(c(gg_filler, bg_filler, gb_filler, bb_filler),
        main='Predicted Probability of Choosing Test Word in Noisy HG',
        xlab='Conditions of Test Word',
        ylab='Probability of Being Chosen Against Filler',
        names.arg=c('GG', 'BG', 'GB', 'BB'))
@

<<ME-prob, echo=FALSE, eval=FALSE>>=
ME_prob = function(a, b){
  p_a = exp(-a)/(exp(-a)+exp(-b))
  p_b = exp(-b)/(exp(-a)+exp(-b))
  return(p_a)
}
ME_gg = ME_prob(0, 1)
ME_bg = ME_prob(2, 1)  
ME_gb = ME_prob(3, 1)
ME_bb = ME_prob(5, 1)
barplot(c(ME_gg, ME_bg, ME_gb, ME_bb))
# noisy hg is more winner take all
barplot(c(ME_gg, gg_filler, ME_bg, bg_filler, ME_gb, gb_filler, ME_bb, bb_filler))

@

<<linear-ME-prob, echo=FALSE, eval=FALSE>>=
gg_l = 0/-1
bg_l = -2/-3
gb_l = -3/-4
bb_l = -5/-6
print(bg_l + gb_l)
print(bb_l)
barplot(c(gg_l, bg_l, gb_l, bb_l))

z = 1 + 2 + 3 + 5
gg_l2 = 0
bg_l2 = -2/z
gb_l2 = -3/z
bb_l2 = -5/z
print(bg_l2 + gb_l2)
print(bb_l2)
barplot(c(gg_l2, bg_l2, gb_l2, bb_l2))
@

I predict that, in accordance with the Maximum Entropy model, additional violations
will have smaller effects on the grammaticality of the word, as the grammaticality approaches a floor.


%In order to simplify this investigation, I will not attempt to define the set of patterns
%that can be considered phonotactic violations. Rather, I will use controlled experiments
%dealing only with patterns that are widely accepted to belong to that set for the language
%in question.

\section{Method}

In order to test the prediction made by the Maximum Entropy model, I gathered acceptability data on words much like the ones used in the examples above: words with no obvious violations, words that are the same except with the addition of a violation in the onset, words the same as the first group except with a violation in the coda, and words with both the onset and coda violation. This will allow us to distinguish between the Harmonic Grammar and Maximum Entropy models of the cumulativity of constraint violations.


%Relative to words with one violation, does the same violation add less, the same amount, or
%more ungrammaticality to the word when it appears in a word that already has
%another violation?

% Luce choice rule: 

% \ex. P(test) = \mathcal{H}_{test}/\mathcal{H}_{test} + \mathcal{H}_{filler}

\subsection{Participants}
One hundred participants were recruited from Mechanical Turk and paid for their participation at prorated minimum wage. They were all located in the United States and claimed to be over 18 years old. In order to maintain a level of consistency in the participant pool, I ran the experiment only on weekdays between the hours of noon and 5pm Eastern time, which corresponds to regular workday hours in the four major US timezones.

Participants were excluded if they were not native speakers of English and if their data was suspect. Native status was determined by answers to two demographics questions: one asking the language they use at home and one asking where people think they are from. I read the answers to these questions and excluded participants whose answers indicated that they are not native speakers of English. Two participants were excluded on the basis of native language.

The quality of the data was assessed in a variety of ways.

First, there were twelve filler trials. These had the same form as test trials, a yes/no question about the acceptability of a nonce word in English. However, six words were constructed to be more English-like than even the best test words, and six were constructed to be less English-like than even the worst test words. Participants were excluded if they accepted good fillers equally as often as bad fillers, indicating that they may not have been paying attention or answering carefully. Four participants were excluded for this reason. No participants accepted bad fillers more often than good fillers.

Second, I decided to exclude any participant who consistently chose whichever option was on a particular side of the screen. I defined consistently as more than 90\% of test questions. No participants fell into this category.

The third exclusion criterion was answer speed. I would exclude any participant who answered any question in under 50ms. In pilot data, the fastest reaction times were over 300ms, so times under 50ms suggest that a computer program is clicking through the experiment automatically. No participants were found to have this behavior.

After these exclusions, the data from 94 participants was used in the analysis.

\subsection{Materials}
% bad codas: 
%fricative noncoronal – needs long vowel
%fricative non-s fricative
%nasal place-a obs place-b
%not: stop non-coronal-stop
%not: obs son

% bad onsets:
%fricative liquid
%bw – unattested
%dw, gw, pw – low frequency


%TODO list them in appendix
The words were presented orthographically. The benefit of visual presentation is that participants are much less likely to fail to perceive the violations. There is a large body of evidence that speakers misperceive certain sound combinations that would be severe phonotactic violations in their native language. The result is that their behavioral data does not reflect the presence of the violation \citep{
berent_what_2007, % perceptual repairs aren't phonetic; Russians are more accurate, English speakers can be accurate under the right conditions
berent_listeners_2009-1, % m@l- vs ml-, English speakers repair
%breen_perceptual_2013, % tl- vs gl-, no repair in ERP but repair in behavior
brown_expectancy_1956, % more errors on ungrammatical clusters than unattested ones
dupoux_epenthetic_1999, % eb(u)zo, Japanese speakers repair
%dupoux_new_2001, % manipulate perceptual repairs to support prelexical processing theory
%halle_dental--velar_2007, % French and American speakers worse at tl/gl than Hebrew
%halle_perception_2003, % French worse at tl/gl than Hebrew
halle_processing_1998, % French get bad at tl/gl when they hear more of the l
massaro_phonological_1983% many kinds of perceptual repairs
%moreton_structural_2002-1, % compared perceptual error rates to support feature theory
}. If some violations were not perceived, the results of the experiment would be compromised, and indeed, some of the violations used in this study, such as [tl], are known to be among those misperceived by English speakers \citep{
breen_perceptual_2013, % tl- vs gl-, no repair in ERP but repair in behavior
halle_dental--velar_2007% French and American speakers worse at tl/gl than Hebrew
}. The downside of visual presentation is that we are testing orthotactics more directly than phonotactics. The materials were designed to minimize the ambiguity in the relationship between spelling and sound in order to mitigate this problem as much as possible.

\subsubsection{Test Items}
Each of 24 test items appeared in four conditions. The four conditions are created by
crossing two factors: presence of an onset violation and presence of a rime
violation. 

For each item, there is a unique good onset, bad onset, good coda, and bad coda. There is also a vowel assigned to the item. The four conditions of the item are created by taking all combinations of one onset, the vowel, and one coda.

The good onsets are all biconsonantal or triconsonantal and attested in native English words. The bad onsets are all biconsonantal, though some are represented by three letters, such as \textit{thl}, as the materials were presented orthographically. The bad onsets are not attested in native English words, although some are found in foreign proper nouns, such as \textit{Sri Lanka}, \textit{Vladimir}. 

The good codas consist of one consonant, sometimes spelled with two letters, such as \textit{ss} or \textit{ck}. The bad codas are biconsonantal. These codas are not attested in native English words, and are believed to violate English phonotactics. Many of them have sonority profiles not allowed in English, but extremely bad sonority profiles --- those with a sonorant followed by an obstruent --- were avoided to keep participants from mentally inserting a schwa into the coda cluster, breaking
it into two syllables.

The vowels are taken from the set {\textit{a}, \textit{e}, \textit{i}, \textit{o}, \textit{u}, \textit{oo}}, in order to have a uniform distribution of orthographically distinct vowels across the items.

Items were created randomly from these components, and then altered to avoid any actual English words.

\ex. Example test item
\a. Good onset, good coda: plag  	
\b. Good onset, bad coda: plavb  
\c. Bad onset, good coda: tlag  
\d. Bad onset, bad coda: tlavb  

\subsection{Design}
A Latin square design is applied to the test items so that each participant only sees one
word from each item set. 

\subsection{Procedure}
The experiment was built using Speriment and run using psiTurk \citep{PsiTurk} to interact with Mechanical Turk.

The task is a two-alternative forced choice task between a test word and a filler word, for 24 trials. In each trial, the participant was asked to choose the more English-like of the two words. 

Participants indicated their choice by pressing a key on their keyboard: `f' for the choice on the left and `j' for the choice on the right. The order in which the test and filler words were presented varied randomly across items and participants.

\section{Results}

<<cumulativity-dataframe, echo=FALSE, warning=FALSE>>=
cdata = read.csv('~/dissertation/cumulativity/cumulativity_june3_dataframe.csv')
# mean of Response is number of times test word chosen / total number of questions
test_groups = aggregate(cdata, by=list(cdata$Condition), FUN = mean, na.rm = TRUE)
subject_groups = aggregate(cdata, by=list(cdata$Condition, cdata$Subject), FUN = mean, na.rm = TRUE)
item_groups = aggregate(cdata, by=list(cdata$Condition, cdata$Item), FUN = mean)
#rtdata = cdata[cdata$ReactionTime < 20000,]
rtdata = cdata
subject_rt_groups = aggregate(rtdata, by=list(rtdata$Condition, rtdata$Response, rtdata$Subject), FUN = mean)
item_rt_groups = aggregate(rtdata, by=list(rtdata$Condition, rtdata$Response, rtdata$Item), FUN = mean)
get_percentages = function(df){
  return(list(df[df$Group.1 == 'GG',]$Response * 100,
              df[df$Group.1 == 'BG',]$Response * 100,
              df[df$Group.1 == 'GB',]$Response * 100,
              df[df$Group.1 == 'BB',]$Response * 100))
}

get_rts_log = function(df){
  return(list(log(df[df$Group.1 == 'GG' & df$Group.2 == 1,]$ReactionTime),
              log(df[df$Group.1 == 'GG' & df$Group.2 == 0,]$ReactionTime),              
              log(df[df$Group.1 == 'BG' & df$Group.2 == 1,]$ReactionTime),              
              log(df[df$Group.1 == 'BG' & df$Group.2 == 0,]$ReactionTime),              
              log(df[df$Group.1 == 'GB' & df$Group.2 == 1,]$ReactionTime),              
              log(df[df$Group.1 == 'GB' & df$Group.2 == 0,]$ReactionTime),              
              log(df[df$Group.1 == 'BB' & df$Group.2 == 1,]$ReactionTime),             
              log(df[df$Group.1 == 'BB' & df$Group.2 == 0,]$ReactionTime)))
}

get_rts = function(df){
  return(list(df[df$Group.1 == 'GG' & df$Group.2 == 1,]$ReactionTime,
              df[df$Group.1 == 'GG' & df$Group.2 == 0,]$ReactionTime,              
              df[df$Group.1 == 'BG' & df$Group.2 == 1,]$ReactionTime,              
              df[df$Group.1 == 'BG' & df$Group.2 == 0,]$ReactionTime,              
              df[df$Group.1 == 'GB' & df$Group.2 == 1,]$ReactionTime,              
              df[df$Group.1 == 'GB' & df$Group.2 == 0,]$ReactionTime,              
              df[df$Group.1 == 'BB' & df$Group.2 == 1,]$ReactionTime,             
              df[df$Group.1 == 'BB' & df$Group.2 == 0,]$ReactionTime))
}

global_percentages = get_percentages(test_groups)
subject_percentages = get_percentages(subject_groups)
item_percentages = get_percentages(item_groups)

subject_rts = get_rts_log(subject_rt_groups)
item_rts = get_rts_log(item_rt_groups)

@

As expected under any model, participants accepted the test item the most often when it had no violations, less often when it had one violation, and very rarely when it had two violations. Of the two types of words containing one violation, those with a coda violation and those with an onset violation, coda violation words were chosen less often, suggesting that the coda violations were on average more egregious than the onset violations. Thus, there is a total ordering of word types, from those with no violations (called GG for good onset, good coda), to those with an onset violation (BG for bad onset, good coda), to those with a coda violation (GB for good onset, bad coda), to those with two violations (BB for bad onset, bad coda).

The question this experiment seeks to answer is not about the ranking of the percentages for each condition, but the quantitative relationships among them. In order to analyze these relationships, we need a linking hypothesis to map from predictions about psychological states to predictions about performance on the task. I hypothesize that participants use the output of whichever
model they are using --- harmony in Harmonic Grammar and probability in Maximum Entropy --- as the input to a probabilistic process that governs whether they choose `yes' or `no' on the task. There are many forms this probabilistic process could take; I will adopt the assumption that the percent of times a participant accepts a word can be treated as a direct proxy for the output of the model for that word. That is, I assume that if participants calculate probabilities for words, they say `yes' to a word with the same probability they assign to the word, and if they calculate harmonies, they say `yes' with a probability that is proportional to the harmony the assign the word. I assume that the scaling necessary to convert harmonies to probabilities in the case of Harmonic Grammar is constant across words, so that the differences in the percents of `yes' answers is the same, modulo the noise of the probabilistic process, as the differences in the harmonies. This is equivalent to normalizing the harmonies for all stimuli for a given subject and item set consistently. With our current limitations in understanding the transformations that apply to model outputs as they are used to direct behavior in an experimental task, the results of this experiment must be interpreted as dependent on this assumption. A more conclusive understanding of the phenomenon will depend on future studies that investigate it using different tasks to determine if the findings are dependent on a particular linking hypothesis, or if they are robust to different ways of framing the question and to the particulars of different experimental tasks.

The difference between the model outputs of GG words and another category of words can be viewed as the penalty for the violation(s) in the second category of words. 
%This experiment rests on the linking hypothesis that participants choose test words over filler words proportionally to the acceptability of the test words, so I will use percent of the time that a test word was chosen as a proxy for its acceptability.

Harmonic Grammar predicts that the penalty for BB is equal to the sum of the penalties for BG and GB, while Maximum Entropy predicts that the penatly for BB is less than this sum. Table \ref{tab:cumulativity-percents} and Figure \ref{fig:cumulativity-global-percents} show that descriptively, the data appear to support the prediction of a Maximum Entropy model, as the penalties decrease at each step. Both report the percent of the time that participants responded `yes' to a word, for each type of word.

%TODO round numbers
\begin{table}
\begin{center}
\caption{Percent acceptance by condition.}
\begin{tabular}{lll}
\hline Condition & Percent `Yes' \\
\hline GG & \Sexpr{global_percentages[[1]]} \\
BG & \Sexpr{global_percentages[[2]]}\\
GB & \Sexpr{global_percentages[[3]]}\\
BB & \Sexpr{global_percentages[[4]]}\\
\hline
\end{tabular}
\label{tab:cumulativity-percents}
\end{center}
\end{table}

\begin{figure}
\begin{center}
<<cumulativity-global-percents, echo=FALSE>>=
barplot(c(global_percentages[[1]], global_percentages[[2]], global_percentages[[3]], global_percentages[[4]]),
        main = 'Percent of Yes Responses By Condition',
        names.arg = c('GG', 'BG', 'GB', 'BB'),
        xlab = 'Condition of Test Word',
        ylab = 'Percent of of Yes Responses')
@
\caption{Percent acceptance by condition.}
\label{fig:cumulativity-global-percents}
\end{center}
\end{figure}

Bar plots don't show the shape of the distributions, though, so we can also break down the data by subject and item in order to view violin plots of the conditions. A violin plot is a box plot where the sides of the boxes are replaced with kernel density plots. Thus, the white dots represent the median subject or item mean, the black bars represent the interquartile range --- the values in the medial two quartiles of the data. Figure \ref{fig:cumulativity-subject-percents} is a violin plot where each data point is the percent of `yes' answers given to that word type by a particular participant. It shows us the distribution of participants for each condition. Figure \ref{fig:cumulativity-item-percents} is a similar plot where the aggregation is done by item set. Recall that an item set is a set of four test words, one in each condition, where the words share common subsequences and constraint violations, where violations exist.
 
\begin{figure}
\begin{center}
<<cumulativity-subject-percents, echo=FALSE, message=FALSE>>=
library("vioplot")
make_vioplot = function(percentages, header){
  vioplot(percentages[[1]], percentages[[2]], percentages[[3]], percentages[[4]], 
          names = c('GG', 'BG', 'GB', 'BB'),
          col='blue')
  title(main = header,
          xlab = 'Condition of Test Word',
          ylab = 'Percent of Times Test Chosen Over Filler')
}
make_vioplot(subject_percentages, 'Preference for Test Words By Subject')
@
\caption{Distribution of mean by-subject percent acceptance for each condition.}
\label{fig:cumulativity-subject-percents}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
<<cumulativity-item-percents, echo=FALSE>>=
make_vioplot(item_percentages, 'Preference for Test Words By Test Word')
@
\label{fig:cumulativity-item-percents}
\caption{Distribution of by-item percent acceptance for each condition.}
\end{center}
\end{figure}

In Figure \ref{fig:cumulativity-subject-percents}, we see that the distribution for BB appears cut off at the bottom. This suggests a floor effect: the BB words are so unacceptable that we cannot get an accurate sense of how unacceptable they are, because we already hit zero percent `yes' responses, and it's not possible to give a negative number of `yes' responses. This result is problematic for the interpretation of this experiment, which hinges on an accurate estimation of the acceptability of BB words. If participants are using an HG-like grammar, could it map to a negative number of `yes' responses, which is then forced up to zero, making it appear that they employed a MaxEnt-like grammar instead? 

One problem with this alternative interpretation is that it's unclear how harmonies would map to a negative number of intended `yes' responses. We would need a more sophisticated linking hypothesis than the one offered above, which assumed that harmonies are transformed into probabilities in a way that preserves their proportions. Implicit in that assumption was the idea that they were normalized to fit into the probability space and translated into positive space, keeping the proportions among them constant. Perhaps they are scaled but not fully pushed into the positive range? But what would it mean for our hypothesized probabilistic process to have a target of a negative number? The fact that harmonies must be shoehorned into a positive space may be the very motivation for a MaxEnt-like model which transforms harmonies and results in the attentuation of differences at the bottom of the scale. Furthermore, we might expect a more dramatic floor effect, and one consistent whether we look at the data aggregated by subject or by item, if indeed the subjects intended to assign BB words a harmony score as low as HG would predict for this data. Finally, we might expect that participants would alter their response rates to the other words in order to maintain the relationships among the words, even though the task does not ask them to do so. There is some preliminary evidence that this is the case; in a small pilot using less severe violations, intended to avoid the floor effect, the floor effect was in fact more extreme than in a pilot with the materials used here. The shapes of the distributions over BB words shows that participants were not perfectly calibrating their response rates to fill the space of possible responses, but the shapes also do not suggest that they were failing to calibrate completely. 

One clue to participants' true assessments of the BB words may lie in their reaction times. If BB words were far worse, not just slightly worse, than BG and GB words, and participants merely ran out of ways to express this, we might expect them to have shorter reaction times in responding to BB words than to BG and GB words, because it was so obvious that they should be rejected. In particular, we would expect shorter reaction times for rejecting BB words than for rejecting the second worst category, GB words. In fact, however, the violin plots for log-transformed reaction times for rejected GB and BB words look fairly similar, as shown in Figure \ref{fig:cumulativity-rt-violin}. 

\begin{figure}
\begin{center}
<<cumulativity-rt-violin>>=
rejectgb = rtdata[rtdata$Condition == 'GB' & rtdata$Response == 0,]$ReactionTime
rejectbb = rtdata[rtdata$Condition == 'BB' & rtdata$Response == 0,]$ReactionTime
vioplot(log(rejectgb), log(rejectbb), col='blue')
@
\caption{Distribution of reaction times in rejecting GB and BB words.}
\label{fig:cumulativity-rt-violin}
\end{center}
\end{figure}

\begin{table}
\begin{center}
\caption{Central tendencies of reaction times for GB and BB words.}
\begin{tabular}{lll}
\hline Condition & Mean & Median \\
\hline
Rejected GB & \Sexpr{mean(log(rejectgb))} & \Sexpr{median(log(rejectgb))}\\
Rejected BB & \Sexpr{mean(log(rejectbb))} & \Sexpr{median(log(rejectbb))}\\
\hline
\end{tabular}
\label{tab:cumulativity-rt}
\end{center}
\end{table}

<<cumulativity-rtt>>=
rtt = t.test(log(rejectgb), log(rejectbb), alternative = 'greater')

#rt_model = lmer(log(ReactionTime) ~ Condition + TrialNumber + YesPosition + (1|Subject) + (1|Item), data = rtdata[rtdata$Response == 0,])

#rt_model2 = lmer(log(ReactionTime) ~ Condition + TrialNumber + YesPosition + (1|Subject) + (1|Item), data = rtdata[rtdata$Response == 1,])
@

The mean and median reaction times for BB words are lower than those for GB words, as given in Table \ref{tab:cumulativity-rt}, but a one-tailed t-test finds that the difference is not significant, with a $t$-score of \Sexpr{rtt$statistic} and a $p$-value of \Sexpr{rtt$p.value}. Thus, reaction times do not offer support for the Harmonic Grammar hypothesis.

The hypothesis that participants are employing a MaxEnt-like grammar rather than an HG-like one predicts that there will be an interaction between the effects of onset violations and coda violations. 

If a positive interaction is present, this would support a model that takes probability
away from words in greater and greater amounts as the number of violations increases.
I do not know of such a model, and this result is not predicted.

A negative interaction would support models like Maximum Entropy, in which each additional
violation subtracts less probability from the word than the last violation did.

A linear relationship would support models like linear Harmonic Grammar, in which each
additional violation subtracts the same amount of probability from a word as the last
violation did.

Figure \ref{fig:cumulativity-interaction} shows the interaction of the effects of coda violations and onset violations on `yes' responses. The difference between the slopes of the two lines shows that the addition of a coda violation decreases the acceptance rate less when an onset violation is already present than when it is not, as predicted by Maximum Entropy.

\begin{figure}
\begin{center}
<<cumulativity-interaction, echo=FALSE>>=
with(cdata[cdata$PageType == 'Test',], interaction.plot(CodaViolation, OnsetViolation, Response, fun=mean, legend=TRUE, ylim = c(0,1), type='l'))
@
\caption{Interaction between OnsetViolation and CodaViolation.}
\label{fig:cumulativity-interaction}
\end{center}
\end{figure}

To test the significance of this interaction, a mixed effects model was fitted to the data. The dependent variable
was the percent of `yes' responses. OnsetViolation,
CodaViolation, and their interaction served as fixed effects, in addition to TrialNumber and YesPosition, representing whether the `Yes' option was presented on the left or right side of the screen on a particular trial. Random slopes and intercepts for subject and test item were included in the model.

<<cumulativity-analysis, cache=TRUE>>=
library(lme4)
cdata$OnsetViolation[which(cdata$OnsetViolation == 0)] = -1
cdata$CodaViolation[which(cdata$CodaViolation == 0)] = -1
cdata$TrialNumber = cdata$TrialNumber - mean(cdata$TrialNumber)
cdata$YesPosition = cdata$YesPosition - mean(cdata$YesPosition)
#didn't converge but close, add iterations
full_model = glmer(Response ~ OnsetViolation * CodaViolation + TrialNumber + YesPosition + (1|Subject) + (0+OnsetViolation|Subject) + (0 + CodaViolation | Subject) + (0+ OnsetViolation:CodaViolation|Subject) + (1|Item) + (0+OnsetViolation|Item) + (0 + CodaViolation | Item) + (0+ OnsetViolation:CodaViolation|Item), data = cdata, family = binomial(link="logit"), glmerControl(optCtrl=list(maxfun=10000), optimizer = "bobyqa" ) )
@

Table \ref{tab:cumulativity-coefficients} gives the coefficients of the mixed effects model. It shows significant effects of both OnsetViolation and CodaViolation, meaning that the presence of an onset violation significantly decreases acceptance rate, as does the presence of a coda violation. Additionally, it shows a significant subadditive interaction between the two, showing that the combination of both violations is less potent than would be expected if they acted completely independently. This is in line with the Maximum Entropy prediction.

%TODO
\begin{table}
\begin{center}
\caption{Coefficients of the mixed effects model.}
<<cumulativity-coefficients, cache=TRUE>>=
summary(full_model)$coefficients
@
\label{tab:cumulativity-coefficients}
\end{center}
\end{table}

%TODO
%The MaxEnt prediction is in fact more specific than a subadditive interaction. It is that the acceptance rates of word types is proportional to their log probabilities. Ideally, we would test this more specific hypothesis by fitting a MaxEnt model to the data and comparing the fit to that of other models. The problem with this approach is that a MaxEnt model, as any constraint-based model, requires constraints, and we do not know for sure which constraints ought to be used. If we could decide on a constraint set, we could learn weights for that constraint set and some approximation of the English language, and apply the resulting grammar to the nonce words. 

% see Barr et al p.25 if model doesn't converge

% Barr et al p 27:
% For obtaining p-values from analyses of typically-sized psycholinguistic datasets—where
% the number of observations usually far outnumbers the number of model parameters—our
% simulations suggest that the likelihood-ratio test is the best approach. To perform such a test,
% one compares a model containing the fixed effect of interest to a model that is identical in all
% respects except the fixed effect in question. One should not also remove any random effects
% associated with the fixed effect when making the comparison. In other words, likelihoodratio
% tests of a fixed effect with k levels should have only k − 1 degrees of freedom (e.g., one
% degree of freedom for the dichotomous single-factor studies in our simulations). We have
% seen cases where removing the fixed effect causes the comparison model to fail to converge.
% Under these circumstances, one might alter the comparison model following the procedures
% described above to attempt to get it to converge, and once convergence is achieved, compare
% it to an identical model including the fixed effect. 





% Barr et al p. 28:
%  report the variance-covariance matrix, which includes all the
% information about the random effects, including their estimates. This is useful not only as a
% check on the random effects structure, but also for future meta-analyses. A simpler option is
% to mention that one attempted to use a maximal LMEM and, as an added check, also state
% which factors had random slopes associated with them. If the random effects structure had to
% be simplified to obtain convergence, this should also be reported, and the simplifications that
% were made should be justified to the extent possible.

\section{Discussion}

The results support the hypothesis that Maximum Entropy predicts participants' preferences better than Harmonic Grammar. The interaction between the effect of an onset violation and the effect of a coda violation was both statistically significant and of a considerable effect size. Although the data appears to suffer from a floor effect, the hypothesis that participants are using a Maximum Entropy grammar fits the results better than the hypothesis that they're using Harmonic Grammar.

Several variations on this experiment would be helpful in increasing our confidence in the results. Auditory stimuli would help ensure that we are measuring phonology rather than orthographical effects, although it would be important to find stimuli that can be accurately perceived. Testing speakers of other languages on violations of their phonological constraints would show whether this is a robust effect, and whether it reflects something about human grammar rather than a fact particular to English and the kinds of constraints available for testing on English speakers. Investigations of the cumulativity of constraint violations that rely on different tasks and designs would be helpful in reducing our reliance on the linking hypothesis adopted for this experiment. Replications of this experiment with a variety of violations, especially mild ones, would show whether the floor effect found here is in fact as mild as it seems, or whether it hides a pattern of acceptability ratings that is consistent with Harmonic Grammar. 

% Hayes and Wilson 2008 on Chomsky and Halle 1965
% "All areas of generative grammar that address well-formedness are faced with the problem of
% accounting for gradient intuitions. A large body of research in generative linguistics deals with
% this issue; for example Chomsky 1963; Ross 1972; Legendre et al. 1990; Schütze 1996; Hayes
% 2000; Boersma and Hayes 2001; Boersma 2004; Keller 2005; Sorace and Keller 2005; Legendre
% et al. 2006. In the particular domain of phonotactics, gradient intuitions are pervasive: they have
% been found in every experiment that allowed participants to rate forms on a scale (e.g.,
% Greenberg and Jenkins 1964, Ohala and Ohala 1986, Coleman and Pierrehumbert 1997,
% Vitevitch et al. 1997, Frisch et al. 2000, Treiman et al. 2000; Bailey and Hahn 2001, Hay,
% Pierrehumbert, and Beckman 2003, Hammond 2004) and binary responses yield similar
% responses when averaged across participants (Coleman and Pierrehumbert 1997, Pierrehumbert
% 1994, Pater and Coetzee 2006). Thus, we consider the ability to model gradient intuitions to be
% an important criterion for evaluating phonotactic models. As we will show below, it is an
% inherent property of maximum-entropy models that they can account for both categorical and
% gradient phonotactics in a natural way."

The results of this study cannot, of course, single out Maximum Entropy as the correct grammar. There may be other models that would also fit the data gathered here. However, these findings do pose a challenge to Harmonic Grammar as a model of cumulative phonotactics and support the idea that violations make more of a difference at the high end of the acceptability scale than at the low end. This idea can bear on the question of whether grammaticality is categorical or gradient, which \citet{Hayes2008} point out has been a challenging question for decades. If differences matter more in one region of the scale than another, this can make elicited intuitions appear to show a threshold between grammatical and ungrammatical data, offering a sort of reconciliation between categorical and gradient views of phonotactic well-formedness.

%TODO figure, move to results
Another insight this experiment can bring has to do with the very high region of the acceptability scale, which is often overlooked. The especially acceptable fillers in this experiment, made from violation-free words and real English suffixes, were rated slightly higher than the GG (violation-free) words. This raises challenges for 
